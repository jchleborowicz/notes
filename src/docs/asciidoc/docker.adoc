= Docker
:icons: font
:imagesdir: images

== General

Book: Docker: Up and Running by Karl Matthias & Sean P. Kane

Main challenges:

* management
* security
* certification
Introduced by Solomon Hykes, founder and CEO of dotCloud.

.Benefits

* packaging software in a way that leverages the skills developers already have
* building application software and required OS filesystems together in a single standardized image format
* using packaged artifacts to test and deliver the exact same artifact to all systems in all environments

.What Docker is not

* Enterprise Virtualization Platform (VMware, KVM, etc.)
* Cloud Platform (Openstack, CloudStack, etc.)
* Configuration Management (Puppet, Chef, Ansible etc.)
* Deployment Framework (Capistrano, Fabric, etc.)
* Workload Management Tool (Mesos, Fleet, etc.)
* Development Environment (Vagrant, etc.)

Docker is an open platform for developers and sysadmins to build, ship, and run distributed applications.

Consists of:

* docker engine - a portable, lightweight runtime and packaging tool
* docker hub - a cloud service for sharing applications and automated workflows

Traditional VMs:

* server hardware
* host operating system
* hypervisor (virtual box, vm ware)
    ** and then inside hypervisor there are guest OS
        *** bins/libs/apps

In docker:

* server hardware
* host operating system
* docker engine
    ** inside bins / libs / apps

.Running docker daemon

 $ docker -d

Docker ports:

* 2375 - unsecure
* 2376 - secure

.Tooling:

* Compose
* Machine
* Swarm

.Command line tool

* build a container image
* pull/push images from/to registry
* start container
* retrieve logs from a remote server
* start a command line shell inside a running container

== Container networking

* each container has its own virtual Ethernet interface connected to the Docker bridge and its own IP address allocated to the virtual interface
* docker allows to bind ports on the host to the container
* incoming traffic passes over a proxy before getting to the container

Docker allocates the private subnet from an unused private subnet block.

Network interface on the server called docker0.

All of the containers are on a network together and can talk to each other directly.

To get to the outside world, containers talk to docker0 virtual bridge interface.

.Options for enforcing isolation:
* SELinux
* AppArmor

Docker filesystem layers

* each filesystem layer identified by unique hash.

Docker is tagging images at deployment time

Docker ecosystem:

. Orchestration:
    .. Docker’s Swarm
    .. New Relic’s Centurion
    .. Spotify’s Helios
    .. Google’s Kubernetes
    .. Apache Mesos
. Atomic hosts:
    .. CoreOS
    .. Project Atomic
. Additional tools
    .. Mozilla’s Heka log router

== Terminology

docker client::
    command used to control docker and talk to remote docker servers
docker server::
    the _docker_ command run in daemon mode.
docker images::
    one or more filesystem layers and some important metadata that
    represent all the files required to run a dockerized application. +
    A container will typically have both a name and a tag. +
    The tag is generally used to identify a particular release
    of an image.
docker container::
    a linux container that has been instantiated from a docker image.
atomic host::
    a small, finely tuned operating system image (CoreOS, Project Atomic),
    that supports container hosting and atomic OS upgrades.

== Installation

.Installing on Debian

 $ sudo apt-get update
 $ sudo apt-get install docker.io cgroup-lite apparmor
 $ sudo usermod -a -G docker $USER

.Running Docker daemon

 $ sudo docker -d -H unix:///var/run/docker.sock -H tcp://0.0.0.0:2375

.systemd-based linux

 $ sudo systemctl enable docker
 $ sudo systemctl start docker

.upstart-based linux:

 $ sudo update-rc.d docker.io defaults
 $ service docker.io start

== Get started tutorial

 $ docker run hello-world

 $ docker run [OPTIONS] IMAGE [COMMAND] [ARG…]

 $ docker run busybox /bin/echo "hello world"

A container is a stripped-to-basics version of Linux OS.

An image is software you load into a container.

After running above command Docker:
. checked to see if hello-world image is present
. downloaded the image from the Docker Hub
. loaded the image into the container and "ran" it

.Simple dockerfile

 $ mkdir mydockerbuild
 $ cd mydockerbuild
 $ vim Dockerfile

Dockerfile:
[source,dockerfile]
----
FROM docker/whalesay:latest

RUN apt-get -y update && apt-get install -y fortunes

CMD /usr/games/fortune -a | cowsay
----

Build it:

 $ docker build -t docker-whale .

== Dockerfile build process

 $ docker build -t <image-name> .

 $ docker tag <image-id> jchleborowicz/docker-whale:latest

 $ docker login --username=jchleborowicz --email=mailto:j.chleborowicz@gmail.com[j.chleborowicz@gmail.com]

 $ docker push jchleborowicz/docker-whale

.Removing image from local

 $ docker rmi -f <image-id>

 $ docker rmi -f <image-name>

.Show full image ids

 $ docker images --no-trunc=true

 $ docker inspect <image-name>

=== Running an interactive shell

$ docker run -it ubuntu /bin/bash

* `-i` starts an interactive container
* `-t` creates a pseudo-TTY that attaches stdin and stdout

.Display all containers, also stopped:

 $ docker ps -a

. Giving container a name:

 $ run -d --name my-name <image>

`-d` means detached mode

=== Binding to another host/port

By default docker listens on *unix:///var/run/docker.sock* to allow only local connections by the root user.

 -H tcp://[host:[port][path]

 -H unix://path

== Long-running worker process

 $ JOB=$(docker run -d ubuntu /bin/sh -c "while true do; echo Hello World; sleep 1; done")

 $ docker logs $JOB

 $ docker kill $JOB

.Start a new container

 $ JOB=$(docker run ...)

 $ docker stop $JOB

 $ docker start $JOB

 $ docker restart $JOB

.SIGKILL a container

 $ docker kill $JOB

.Remove a container

 $ docker stop $JOB

 $ docker rm $JOB

=== Bind a service on a TCP port

 $ JOB=$(docker run -d -p 4444 alpine nc -l 4444)

 $ PORT=$(docker port $JOB 4444 | cut -d: -f2)

 $ echo hello world | nc localhost $PORT

To edit and commit an image:

 $ docker run -it <image> /bin/bash

modify sth within a container, and then

 $ docker commit -m "Message" <container-id> <new-image-name>

for example:

 $ docker commit -m "Setting password in custom configuration" 84fd redis-password

to log into docker hub:

 $ docker login

to push image:

 $ docker tag redis-password jchleborowicz/redis password

 $ docker push jchleborowicz/redis-password

== Dockerfile

[source,dockerfile]
----
FROM node:0.10

MAINTAINER Anna Doe <mailto:anna@example.com[_anna@example.com_]>

LABEL "rating"="Five Stars" "class"="First Class"

USER root

ENV AP /data/app

ENV SCPATH /etc/supervisor/conf.d

RUN apt-get -y update

#The daemons

RUN apt-get -y install supervisor

RUN mkdir -p /var/logl/supervisor

#Supervisor configuration

ADD ./supervisord/conf.d/* $SCPATH/

#Application Code

ADD *.js* $AP/

WORKDIR $AP

RUN npm install

CMD ["supervisord", "-n"]
----

=== Building

 $ docker build -t example/docker-node-hello:latest .

NOTE: use --no-cache switch not to use cache

 $ docker run -d -p 8080:8080 example/docker-node-hello:latest

Running with environment variables:

 $ docker run -d -p 8080:8080 -e WHO_ENV_VAR="Jacek" example/docker-node-hello:latest

`docker-registry` project for local docker registries

=== Authenticating to a registry

 $ docker login

Creates ~/.dockercfg file

 $ docker logout

If logging somewhere else than dockerhub, specify host on the command line:

 $ docker login someregistry.example.com

To check what has changed in the filesystem:

 $ docker diff <image>

To copy files:

 $ docker cp <image>:/usr/bin/file .

 $ docker inspect <image>

=== Mirroring a registry

Launch docker with additional parameter: *--registry-mirror*

=== Linux Container

Virtualization systems:

* VMware, Xen
* virtualized layer called `a hypervisor`
* each hosted kernel sits in separate memory space and has defined entry points into the actual hardware

== Linux Containers History

* 1979 - Version 7 Unix - `chroot` system call restricting filesystem access
* 2000 - FreeBSD 4.0 - `jail` command
* 2004 - Solaris 10 - Solaris Containers, which later evolved into Solaris Zones
* 2007 - HP released Secure Resource Partitions for HP-UX, later renamed to HP-UX Containers
* 2008 - LXC released in kernel version 2.6.24

CoreOS Rocket - open source container runtime

== Creating a container

`docker run` performs two things:

* docker create
* docker start

=== Naming a container:

 $ docker create --name="awesome-service" ubuntu:latest

=== Labels

 $ docker run -d --name labels -l deployer=Ahmed -l tester=Asako ubuntu:latest sleep 1000

 $ docker ps -a -f label=deployer=Ahmed

Use `docker inspect` command to view labels on existing container.

./etc/hostname

 $ docker run -it --rm --hostname="mycontainer.example.com" ubuntu:latest /bin/bash

./etc/resolv.conf

 $ docker run -it --rm --dns=8.8.8.8 --dns=8.8.4.4 --dns-search=example.com --dns-search=example2.com ubuntu:latest /bin/bash

Results in following /etc/resolv.conf:

[source]
----
nameserver 8.8.8.8

nameserver 8.8.4.4

search example.com example2.com
----

=== MAC address (Media Access Control)

By default MAC address starts with _02:42:ac:11_ prefix

 $ docker run -it --rm --mac-address="a2:11:aa:22:bb:33" ubuntu:latest /bin/bash

=== Storage volumes

 $ docker run -it --rm -v /mnt/session_data:/data ubuntu:latest /bin/bash

==== Running read-only

 $ docker run -it --rm --read-only -v /mnt/session_data:/data ubuntu:latest /bin/bash

=== Resource quotas

==== CPU shares

1024 - total shares

512 - 50% of total shares

 $ docker run --rm -ti progrium/stress --cpu 2 --io 1 --vm-bytes 128M --timeout 120s

to run same with half of available CPU time:

 $ docker run --rm -ti -c 512 progrium/stress --cpu 2 --io 1 --vm-bytes 128M --timeout 120s

==== CPU pinning

 $ docker run --rm -ti -c 512 --cpuset=0 progrium/stress --cpu 2 --io 1 --vm 2 --vm-bytes 128M --timeout 120s

==== Constraining memory

 $ docker run --rm -ti -m 512m progrium/stress --cpu 2 --io 1 --vm 2 --vm-bytes 128M --timeout 120s

-m sets both amount of RAM and amount of swap

to set different amount of swap use --memory-swap:

 $ docker run --rm -ti -m 512m --memory-swap=768m progrium/stress --cpu 2 --io 1 --vm 2 --vm-bytes 128M --timeout 120s

to disable swap *--memory-swap -1*

=== ulimits

Defaults for starting all containers with a hard limit of 150 open files and 20 processes:

 $ sudo docker -d --default-ulimit nofile=50:150 --default-ulimit nproc=10:20

Overriding default values:

 $ docker run -d --ulimit nproc=100:200 nginx

== Starting a Container

 $ docker create -p 6379:6379 redis:2.8

 $ docker start 768a93cab55fcd165296614f27d8c9ced2403b22c427ba2dd0172ffb7a21072c

== Auto-restarting a container

 $ docker run -ti --restart=on-failure:3 -m 200m --memory-swap=300m ...

--restart arguments:

* no
* always
* on-failure:#

=== Stopping a Container

 $ docker stop <container-id>

when stopping, SIGTERM is sent

 $ docker stop -t 25 <container-id>

SIGTERM is sent immediately, then, after 25 seconds if the process is still alive SIGKILL is sent

==== Killing an container

 $ docker kill <container-id>

To send any other signal:

 $ docker kill --signal=USR1 <container-id>

==== Pausing a container

 $ docker pause <container-id>

 $ docker unpause <container-id>

==== Removing a container

 $ docker rm <container-id>

To remove all the containers:

 $ docker rm $(docker ps -aq)

==== Removing and image

 $ docker images

 $ docker rmi <image-id>

== Docker video tutorial

Docker file

[source,dockerfile]
----
FROM ubuntu:14.04

MAINTAINER Jacek Chleborowicz <mailto:j.chleborowicz@gmail.com[_j.chleborowicz@gmail.com_]>

#copies a file from source directory into a docker image:

ADD nginx_signing.key /tmp/nginx_singing.key

RUN apt-key add /tmp/nginx_signing.key
----

NOTE: each RUN creates new image

Links:

 $ docker run -p 8000:8000 -d --name django --link postgres:db --link memcached:cache

todo AWS beanstalk !!!

.Interesting projects:

* Flynn
* Deis

Volumes:

* VOLUME instruction inside Dockerfile, or
* -v flag to docker run command

VOLUME /data

 $ docker run -v /data test/webserver

 $ docker run -v /host/dir:/container/dir test/webserver

[[underlying-technologies]]
== Underlying technologies

Docker uses _runc_ driver:

* cgroups - managing resources used by a container (CPU and memory usage). Responsible also for freezing and unfreezing containers
* namespaces are responsible for isolating containers (filesystem, hostname, users, networking and processes)

Lib container also supports SElinux and AppArmor, which can be enabled for tighter security.

UFS - Union File System:

* AUFS
* devicemapper
* BTRFS
* Overlay

The build context:

the docker _build_ command requires a Dockerfile and a _build context_.

The build context is the set of local files and directories that can be referenced from ADD or COPY instructions in the Dockerfile.

.dockerignore file:

.git

*/.git

*/*/.git

*.sw?

[[caching]]
== Caching

* when same parent image and exactly same instruction
* in case of COPY and ADD instructions, the cache will be invalidated if the checksum or metadata for any of the files has changed

[[dockerfile-commands]]
=== Dockerfile commands

ADD /file

CMD

COPY ["src", "dest"]

ENTRYPOINT ["/bin/bash", "-c"]

ENV MY_VERSION 1.3

EXPOSE - indicates to Docker that the container will have a process listening on the given port or ports. This information is used when linking containers or publishing ports by "-P" argument to run; by itself the EXPOSE instruction will not affect networking.

FROM

MAINTAINER

ONBUILD - specifies the instruction to be executed later, when teh image is used as the base layer to another image.

RUN - runs the given instruction inside the container and commits the result

USER - sets the user (by name or UID) to use in any subsequent RUN, CMD or ENTRYPOINT instructions.

VOLUME - declares specified file or directory to be a volume.

WORKDIR - sets the working directory for any subsequent RUN, CMD, ENTRYPOINT, ADD or COPY instructions

$ docker inspect -f \{\{.Author}} IMAGE

[[connecting-containers-to-the-world]]
=== Connecting Containers to the World

$ ID=$(docker run -d -P nginx)

$ docker port $ID 80

[[linking-containers]]
== Linking containers

--link CONTAINER:ALIAS

To prevent containers that haven’t been linked from communicating:

--icc=false

--iptables

[[volumes]]
== Volumes

Docker volumes are directories that are not part of the container’s UFS.

$ docker inspect -f {{.Mounts}} PROCESS

Three way to set up volumes:

1. docker run *-v /data* debian
2. in Dockerfile:
+
[source]
VOLUME /data
3.  docker run parameter: *-v HOST_DIR:CONTAINER_DIR*

[[sharing-data]]
=== Sharing data

 $ docker run -it -h NEWCONTAINER *--volumes-from container-test* debian /bin/bash

All volumes from container container-test will be visible in new container.

.initializing data container:

 $ docker run --name dbdata postgres echo "Data-only container for postgres"

 $ docker run -d --volumes-from dbdata --name db1 postgres

[[deleting-volumes]]
=== Deleting volumes

* container was deleted with *docker rm -v* , or
* the --rm flag was provided to docker run

and:

* no existing container links to the volume
* no host directory was specified for the volume

docker run parameters:

* `-a`, `--attach`
* `-d`, `--detach`
* `-i`, `--interactive`
* `--restart` parameters:
    ** `no` - will never attempt to restart a container
    ** `always`
    ** `on-failure` - takes optional number of times to attempt to restart before giving up:
+
----
docker run --restart on-failure:10 postgres
----
* `--rm` automatically removes the container when it exits. cannot be used with `-d`
* `-t`, `--tty` - allocates a pseudo-TTY
* `-e`, `--env`
+
----
$ docker run -e var1=val -e var2="val 2" debian env
----
* `--env-file`
* `-h`, `--hostname` - sets the container’s unix hostname
* `--name` - assigns name to the container
* `-v`, `--volume`
* `--volumes-from` - mounts volumes from other containers
* `--expose` - only really makes sense with `-P` option
* `--link` - set up a private network interface to the specified container
* `-p`, `--publish`
* `-P`, `--publish-all`
* `--entrypoint`
* `-u`, `--user` sets the user that commands are run under.
* `-w`, `--workdir` sets the working directory in the container.

Managing containers:

* `docker attach`
* `docker create`
* `docker cp`
* `docker exec`
* `docker kill`- by default sends a SIGKILL, the signal can be specified with -s argument
* `docker pause`- suspends all processes. Uses cgroups freezer functionality.
* `docker unpause`
* `docker restart`
* `docker rm`
* `docker start`
* `docker stop`

To detach from container without stopping it press `ctrl+p` `ctrl+q` - works only when attached in interactive mode
with a tty

Docker info

* docker info
* docker help
* docker version

Container info:

* `docker diff` - shows changes made to containers filesystem compared to image it was launched from
* `docker events`
* `docker inspect` - detailed info on containers or images
* `docker logs`
* `docker port` - lists exposed port mappings:
  ** `docker port $ID`
  ** `docker port $ID 6379`
  ** `docker port $ID 6379/tcp`
* `docker ps`
* `docker top` provides information on the running processes inside a given container

Dealing with images:

* `docker build`
* `docker commit -a "Jacek" -m "Comment" $ID commit:test`
* `docker export` - exports to file
* `docker history` - information on each of the layers in an image
* `docker images provides a list of local images
* `docker import`:
+
----
$ docker export 35d17 | docker import - flatten:test +
$ docker history flatten:test
----
* `docker load`
* `docker rmi`
* `docker save`
* `docker tag`:
+
----
$ docker tag faa2b newname +
$ docker tag newname:latest amauat/newname +
$ docker tag newname:latest amouat/newname:newtag +
$ docker tag newname:latest myregistry.com:5000/newname:newtag
----

Using the Registry

* `docker login`
* `docker logout`
* `docker pull`
* `docker push`
* `docker search`

=== Docker compose

Commands:

* up
* build
* ps
* run
* logs
* stop
* rm

=== Docker swarm

* `docker swarm init`
* `docker stack deploy -c docker-compose.yml stack-name`
+
On subsequent calls to `docker stack deploy` docker will update stack.

* `docker stack rm <stack-name>` - removes the stack
* `docker swarm leave --force`

Swarm managers vs swarm workers

----
$ docker-machine create --driver virtualbox myvm1
$ docker-machine create --driver virtualbox myvm2

$ docker-machine ls

$ docker-machine ssh myvm1 "docker swarm init"

$ docker-machine scp docker-compose.yml myvm1:~

$ docker-machine ssh myvm1 "docker stack deploy -c docker-compose.yml getstartedlab"
----

The nodes in a swarm participate in an ingress routing mesh.

Ingress routing mesh ensures that a service deployed at a certain port within
your swarm always has that port reserved to itself, no matter what node
is actually running the container.

To use the ingress network in the swarm, you need to have following ports
open between the swarm nodes before you enable swarm mode:

* 7945 TCP/UDP for container network discovery
* 4789 UDP for the container ingress network

== Networking with docker

----
$ docker network ls
----

Network named *bridge* is a special network.
Docker launches containers in bridge network by default.

----
$ docker network inspect bridge

$ docker network disconnect <network-name> <container-name>
----

*Bridge networks vs overlay networks*

Bridge network is limited to a single host running Docker Engine.
An overlay network can include multiple hosts and is a more advanced topic.

----
$ docker network create -d bridge <network-name>

$ docker run -d --net=<network-name> --name db training/postgres

$ docker inspect --format=’\{\{json .NetworkSettings.Networks}}’ db

$ docker network connect <network-name> <container-name>
----

== Docker swarm

Swarm::
  cluster of docker engines, used to deploy services.

Node::
 instance of Docker engine participating in the swarm.

Manager node::
 accepts service definitions. Dispatches units of work called *tasks*.
+
Manager nodes elect a single leader to conduct orchestration tasks.

Worker nodes::
 receive and execute tasks. Manager nodes can be configured to be manager-only nodes.

Service::
 the definition of the tasks to execute on the worker nodes.
 * replicated services - swarm manager distributes specific number of replica tasks
 * global services - swarm runs one task for the service on every available node in the cluster

Task::
 Carries a Docker container and the commands to run inside the container.
 Manager nodes assign tasks to worker nodes.
+
Once a task is assigned to a node it cannot move
to another node - it can only run on assigned node or fail.

Ingress load balancing::
 Exposes the services you want to make available externally to the swarm. +
 Swarm manager can automatically assign a *PublishedPort* (range 30000-32767)
+
External components can access the service on the PublishedPort
of any node in the cluster (whether or not the node is currently running
the task for the service).
+
All nodes in the swarm route ingress connections to a running task instance.

Swarm mode has an internal DNS component that automatically assigns
each service in the swarm a DNS entry.

The swarm manager uses *internal load balancing* to distribute requests
among services within the cluster based upon the DNS name of the service.

== Docker networking

The Docker networking architecture is build on a set of interfaces
called the *Container Networking Model (CNM)*.

image:docker/container-networking-model.png[Container Networking Model]

=== CNM Constructs

Sandbox::
  contains the configuration of a container’s network stack.
  This includes management of container interfaces,
  routing table and DNS settings.
+
An implementation of a Sandbox could be a Linux Network Namespace,
a FreeBSD Jail or similar concept.
+
A Sandbox may contain many endpoints from multiple networks.

Endpoint::
 Joins a Sandbox to a Network.

Network::
 Collection of endpoints that have connectivity between them.
 Implementation of the network could be a Linux bridge, a VLAN, etc.

=== Network Drivers

Network Drivers::
  provide the actual implementation that makes networks work.
+
Multiple network drivers can be used on a given Docker Engine
or Cluster concurrently, but each Docker network is only instantiated
through a single network driver.
+
Two broad types of CNM network drivers:
+
* *Native Network Drivers* - native part of the Docker Engine
  and are provided by Docker.
+
There are multiple drivers to choose from that support different
capabilities like overlay networks or local bridges.
+
* *Remote Network Drivers* - network drivers created by the community
and other vendors.


IPAM Drivers::
 Docker has a native IP Address Management Driver that provides
 default subnets or IP addresses for networks and endpoints
 if they are not specified.

== O’Reilly course: Learning Path: Delivering Applications with Docker

What are containers:

* isolated view of processes, user space na file system
* shares host linux kernel

OS virtualization technologies:

* FreeBSD Jails
* Solaris Zones
* LXC

Virtual machines vs containers - vm’s use hypervisor, containers use
container engine (jails, solaris zones, LXC).

Docker properties:

* abstraction on container engines (libvirt, LXC, etc)
* command line and HTTP API
* standardized packaging
* layered image format
* ecosystem of tools and services

Docker host contains:

* docker daemon
* containers
* images

Docker registry - remote service that houses docker images.

Docker machine:

* command line tool to create many managed docker hosts

Docker-compose

* wires bunch of containers together

Docker machine:

* docker-machine create --driver virtualbox dev1
* eval $(docker-machine env dev1)
* docker-machine ip dev1
* docker-machine stop dev1
* docker-machine rm dev1

Linking containers:

----
$ docker run -d -P --name redis redis

$ docker run --link redis ubuntu bash

$ docker ps -l - prints last container

$ docker port <container-name> - lists ports
----

Using `docker stop` will issue SIGTERM signal followed by SIGKILL signal.

`$ docker stop --time 10 <container-name>` - waits 10 seconds between SIGTERM and SIGKILL

=== Restart policy

 $ docker run -d *--restart unless-stopped* ubuntu

=== Logs

 $ docker logs -f <container-name>

 $ docker inspect --format=’\{\{.NetworkSettings.IPAddress}}’ <container-name>

=== Docker images

[source,dockerfile]
----
Dockerfile:

FROM ubuntu:15:10

RUN apt-get install python

RUN pip install flask

ADD app.py

EXPOSE 5000

ENTRYPOINT python app.py
----

Starting with alpine linux:

----
apk update

apk add nodejs

nodejs --version
----

 $ docker commit -m "a comment" <container-id>

Sample Dockerfile:

[source,dockerfile]
----
FROM alpine

MAINTAINER Jacek Chleborowicz <j.chleborowicz@gmail.com>

RUN apk update && apk add nodejs

RUN mkdir average

ADD average.js average/

WORKDIR average

ENTRYPOINT ["node","average.js"]
----

 $ docker build -t tag .

=== Build triggers

In Dockerfile:

ONBUILD used to define instructions to execute in descending build

=== Networking

 $ docker network create --driver bridge my-network

 $ docker run -d -P *--net my-network* --name hello rickfast/hello-oreilly-http

Unique container name can be used to resolve ip address.

=== Net alias

 docker run -d --net dns-test --name dns-test-app *--net-alias dns-alias* rickfast/oreilly-dns-test

=== Volumes

File system inside docker is called *union fs*

==== Data volume containers

Containers to only store data

 $ docker *create -v /usr/local/var/lib/couchdb* --name db-data debian:jessie /bin/true

 $ docker run -p 5984:5984 *-v /usr/local/var/lib/couchdb* --name db1 -d *--volumes-from db-data* couchdb

 $ docker run -p 5985:5984 *-v /usr/local/var/lib/couchdb* --name db2 -d *--volumes-from db-data* couchdb

TODO: examine flocker by clusterhq

=== Docker swarm

Need backing key-value storage mechanism. Options:

* consul
* etcd
* zookeeper

----
$ docker run -d -p 2181:2181 --name zookeeper jplocak/zookeeper
----

=== Extras

Kitematic - UI client to docker

==== Log drivers

Json file log driver - default.

to be explicit:

----
$ docker run *--log-driver=json-file* <container-name>
----

Splunk - popular logging service

----
$ docker run --name splunk -p 8080:8080 -p 8088:8088 -d outcoldman/splunk:6.3.3
----

== Docker swarm

Runned with SwarmKit

nodes

deploy services

Manager node - accepts service definition submissions

Manager node sends *tasks* to worker nodes.

Agent runs on worker nodes.

----
$ docker node ls

$ docker service create --replicas 1 --name helloworld alpine ping wyborcza.pl

$ docker service inspect --pretty helloworld

$ docker service scale helloworld=5

$ docker service rm helloworld

$ docker service create --replicas 3 --name redis --update-delay 10s redis:3.0.6

$ docker service update --image redis:3.0.7 redis
----

docker service create params:

* `--update-delay 5s`
* `--update-parallelism 4` - set maximum number of service tasks that scheduler update simultaneously
* `--update-failure-action` - pause|countinue|rollback

$ docker node update --availability drain worker1

$ docker node inspect --pretty worker1

$ docker node update --availability active worker1

=== Ingress routing mesh

Each node in the swarm accepts connections on published ports for any service running in the swarm, even if there’s no task running on the node.

In order to use ingress network in the swarm, you need following ports open between swarm nodes before you enable swarm mode:

* 7946 tcp/udp for container network discovery
* 4789 udp for the container ingress network

==== Publish a port for a service

 $ docker service create --name my-web --publish 8080:80 --replicas 2 nginx

==== Adding published port

 $ docker service update --publish-add 8080:80 my-web

== Presentation *Docker: Beyond the Basics (CI & CD)* by Sean Kane, 2017-11-13

Swarm - entry level workload management tool (proper ones are Mesos and Kubernetes)

Components:

* Docker client
* Docker server (Docker engine)
* Virtual Machine (Docker CE)
* images
* containers

Linux namespaces:

* Mount (filesystem resoures)
* UTS (host & domain name)
* IPC (shared memory, semaphores)
* PID - process tree
* Network
* User - user and group ids

Control groups (cgroups)

* resource limiting
* prioritization
* accounting
* control

== Kubernetes

Linux academy course "Running Container Clusters with Kubernetes" by Terrence Cox

Safari books online course "Kubernetes Fundamentals" by Sebastien Goasguen

What is Kubernetes:

* is an open-source system for automating deployment, scaling and management of containerized apps
* docker or rkt containers
* open source container cluster manager
* released in July 2015
* Apache License 2.0
* design overview
  ** primitives
    *** provide a method for deployment, maintenance and scalability of container based application clusters
    *** designed to be loosely coupled
    *** easily extensible through an API
  ** building blocks
   *** nodes - aka "minions"
   *** pods
   *** labels
   *** selectors
   *** controllers
   *** services
   *** control pane
   *** API

Google’s white paper __"Large-scale cluster management at Google with Borg"__: https://research.google.com/pubs/pub43438.html[_https://research.google.com/pubs/pub43438.html_]

`Borg` was google container management system.

`Omega` is next version of Borg.

`Kubernetes` is a complete open-source rewrite of Borg.

Docker lineage:

* Borg
  ** Omega
  ** Kubernetes
  ** Mesos
** Cloud Foundry
* cgroups
** OCI - Open Container Initiative
** Docker - rkt/appc
** LXC

cgroups:

* Google developed cgroups when working on Borg and donated cgroups to linux kernel
* constrain containers memory and cpu usage

LXC (Linux Containers):

* use cgroups
* namespaces - to isolate containers

Docker

* uses cgroups
* originally used LXC, but then they used their own library called libcontainer to manage namespaces

Where to find information:

* https://kubernetes.io/[_https://kubernetes.io/_]
* https://www.cncf.io/[_https://www.cncf.io/_] - Cloud Native Computing Foundation - open source foundation that manages kubernetes software
* https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA[_https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA_] - CNCF youtube channel
* https://www.youtube.com/channel/UCZ2bu0qutTOM0tHYa_jkIwg[_https://www.youtube.com/channel/UCZ2bu0qutTOM0tHYa_jkIwg_] - Kubernetes youtube channel
* search for Kubernetes on stackoverflow - https://stackoverflow.com/search?q=kubernetes[_https://stackoverflow.com/search?q=kubernetes_]
* github page for kubernetes - https://github.com/kubernetes/[_https://github.com/kubernetes/_]
* https://github.com/kubernetes-incubator[_https://github.com/kubernetes-incubator_] - community projects that are tied to Kubernetes

Architecture

* Head node - is the brain of Kubernetes
  ** API server
  ** scheduler - to place the containers where they need to go
  ** controller manager - checks that the state of the system is what it should be
  ** etcd - datastore for the state of the system
  ** sometimes:
    *** kubelet
    *** docker
* Worker node
  ** kubelet - Kubernetes’ agent
    *** talks with Kubernetes API server and local docker daemon
  ** kube-proxy - system to manage the iptables in the node
  ** docker

*Look at the head node*

----
$ kubectl get nodes

$ systemctl status kubelet

$ less /etc/systemd/system/kubelet.service.d/10_kubeadm.conf

$ cd /etc/kubernetes/manifests # this directory contains yaml configuration files for etcd, apiserver, controller-manager and scheduler

$ ls -l
----

pod - lowest compute unit in kubernetes

 $ kubectl get pods --all-namespaces

*Look at worker node*

----
$ systemctl status kubelet

$ less /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
----

on a head node:

----
$ kubectl run nginx --image=nginx

$ kubectl expose deployments nginx --port 80 --type NodePort
----

on worker node:

----
$ kubectl get pods

$ kubectl get svc
----

*Installing kubernetes:*

* minikube
* gcloud container clusters - on google container engine
* kubeadm - cli tool by community

todo:

* systemd
* iptables
* etcd
