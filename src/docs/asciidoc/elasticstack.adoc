= Elastic Stack
:doc-root: https://notes.jdata.pl
:toc: left
:toclevels: 4
:tabsize: 4
:docinfo1:
:icons: font

== Basics

Components:

[options="header",cols="6,4a",width="50%"]
|====
|Service            |Components
|Core Services      |* Elasticsearch
                     * Logstash
                     * Kibana
                     * Beats
|X-Pack             |* Security
                     * Alerting
                     * Monitoring
                     * Reporting
                     * Graph
                     * Machine Learning
|Elastic Cloud      |
|Elastic Cloud Enterprise (ECE) |
|====

Purpose:

* monitoring

Flow:

. Web Access log
. *Filebeat* - Web Aspect Prospector
. *Logstash* - Web Access Pipeline
. **Elastisearch cluster**
. *Kibana* - search and visualisation

=== Beats

[cols="1d,8a",options="header",grid="cols",width="100%"]
|========================================================================================
|Type           |Features
|Filebeat       |* multiline
                 * include/exclude filters
                 * backpressure tolerant
                 * container ready
|Metricbeat     |* one client, many modules (ex. system, docker, mongodb, kubernetes,
                   nginx, mysql, postgresql)
                 * container ready
|Packetbeat     |* decodes network protocols (icmp, dns, http, amqp, cassandra,
                   mysql, postgresq, thrift-rpc, mongodb, memcache, tls)
                 * correlates requests with responses
|Heartbeat      |* pings via icmp, tcp and http
                 * supports tls, authentication and proxies
                 * simple dns resolution
|Winlogbeat     |* attach to windows event log channels
|Auditbeat      |* auditd module:
                     ** linux only
                     ** attaches to linux kernel's audit framework
                     ** combines multiple audit messages into a single event
                     ** configure what syscalls to monitor
                 * file integrity module:
                     ** compatible with linux, macos, windows
                     ** hashes specified files and monitors changes in real-time
                     ** compare to known malicious file hashes
|========================================================================================

Libbeat - common library, foundation of every beat

=== Logstash

Designed for log processing.

[options="header",cols="1,6"]
|========================================================================================
|Pipeline   |Description

|inputs     |Ingest from multiple sources.
|filters    |Parse events into structured fields, enrich events dynamically.
|outputs    |Stash the processed events, output to multiple destinations
|========================================================================================

=== Elasticsearch

. the heart of the elastic stack
. distributed, RESTful search and analytics engine
. highly scalable adn fault tolerant
. near real time (NRT)
. common use cases:
    .. product search with autocomplete for websites
    .. mine log or transaction data for trends, statisticts or anomalies
    .. quickly investigate, analyze, visualize, and ask ad-hoc questions on huge datasets

==== Important terms

[options="header",cols="1,8a"]
|========================================================================================
|Term       |Description

|Cluster    |* collection of one or more nodes
             * federated searching and indexing across all nodes
             * identified by unique name

|Node       |* single server in the cluster
             * identified by name

|Index      |* collection of documents

|Document   |* basic unit of information
             * expressed in JSON

|Shard      |* piece of an index
             * horizontally splits an index for scalability
             * replication via replica shards:
                 ** replicas are never allocated on the same node as the primary shard
                 ** allows for fault tolerance
                 ** scale search throughput
|========================================================================================

==== Cluster states

[options="header",cols="1,6a"]
|========================================================================================
|State      |Description

|Green      |* all primary shards are allocated
             * all replica shards are allocated

|Yellow     |* all primary shards are allocated
             * one or more replicas are unallocated

|Red        |* one or more primary shards are unallocated
|========================================================================================

==== Node types

[options="header",cols="1,8a"]
|========================================================================================
|Node type              | Description

|Master-Eligible Node   |* Responsible for cluster management:
                             ** Creating/deleting indexes
                             ** Tracking cluster members
                             ** Shard allocation

|Data Node              |* contains shards
                         * handles CRUD, search and aggregation operations

|Ingest Node            |* Executes pre-processing pipelines

|Coordinating-Only Node |* smart load balancer:
                            ** routes requests
                            ** handles search deducing
                            ** distributes bulk indexing
|Machine Learning Node  |* X-Pack machine learning plugin
                         * runs machine learning jobs
                         * handles machine learning API requests
|========================================================================================

==== Best practices

. dedicated nodes for each role
. data node sizing:
    .. 32GB max heap
    .. at least as much free memory as heap memory
    .. solid-state drives
    .. more cores are always better than faster clock speeds
. search and index against coordinating-only nodes
. size to YOUR use case:
    .. load test yoru specific use case and make sizing adjustments
       as necessary
    .. every use case has its own requirements
    .. there is no one-size-fits-all elasticsearch cluster

=== Kibana

. search, view and interact with data stored in elasticsearch
. discover:
    .. interactively explore data
    .. search and filter the data to view specific documents
    .. save searches for quick use later or to embed in dashboards
. visualize:
    .. visual representation of the data
    .. create dashboards
    .. charts, data tables, markdown, gauges, maps and more
. Timelion:
    .. Time series data visualizer
    .. Simple expression language
. Dev tools:
    .. console
        ... UI to interact with the REST API
    .. profiler
        ... visualize API
        ... inspect and analyze seach queries
        ... debug and diagnese performance issues
    .. Grok Debugger:
        ... build and debug grok patterns
        ... same grok implementation as ingest node and Logstash
. Management:
    .. configure index patterns to connect ES indexes to Kibana
    .. manage fields by adding intelligent formatting
    .. configure advanced Kibana options
    .. manage saved searches, visualisations and dashboards

=== X-Pack

Needs license.

Plugins:

[options="header",cols="1,8a"]
|========================================================================================
|Plugin         |Description

|Security       |* Active Directory, LDAP, and SAML authentication
                 * manage user and roles
                 * entcrypt cluster communication with SSL/TSL
                 * index, document and even field level restriction
                 * audit log

|Alerting       |* identify and alert on changes in the data
                 * notifications by email, PagerDuty, Slack, HipChat or webhook

|Monitoring     |* track ES performance and usage at the cluster, node and index level
                 * track Kibana usage and request performance
                 * monitor Logstash throughput and pipeline performance
                 * multi-stack support to centralize ES monitoring

|Reporting      |* generate, schedule and share reports
                 * export raw documents, searches, visualizations
                   and whole dashboards
                 * send reports on a schedule or trigger a report
                   when certain conditions are met

|Graph          |* discover relations in the data
                 * integrate with your apps using the API
                 * visualize in Kibana
                 * filter relevant relationships from popular ones

|Machine Learning   |* unsupervised anomaly detection
                     * visualize and analyze anormalies quickly
                     * prepare for the future with on-demand forecasting
                     * intuitive UI for easy job creation

|========================================================================================

=== Hosting options

[options="header",cols="1,6a"]
|========================================================================================
|Option                 |Description
|Elastic Cloud          |* Hosted ES and Kibana on AWS and GCP
                         * Automated scaling and upg
                         * includes X-Pack
                         * cluster backups every 30 minutes
                         * monitored 24/7

|Elastic Cloud Enterprise |* build and manage ES, Kibana and X-Pack
                             on any infrastructure from one console
                           * manage multiple use cases with separate stacks
                           * offer ES and Kibana as a service to your organization
                           * control your infrastructure

|========================================================================================

== Installing ES on CentOS

[source,bash]
----
$ sudo yum install java-1.8.0-openjdk -y
$ sudo rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch
$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.2.3.rpm
$ sudo rpm --install elasticsearch-6.2.3.rpm
$ sudo systemctl daemon-reload
$ sudo systemctl enable elasticsearch.service
----

Edit `/etc/elasticsearch/elasticsearch.yml`:

[source,yaml]
----
node.name: master1
node.data: false

network.host: ["localhost", "172.31.100.201"]
----

[source,bash]
----
$ sudo systemctl start elasticsearch
$ sudo systemctl status elasticsearch
----

Logs - `/var/log/elasticsearch/`:

* elasticsearch_deprecation.log
* elasticsearch_index_indexing_slowlog.log
* elasticsearch_index_search_slowlog.log
* elasticsearch.log

Configuring data node (elasticsearch.yml):

[source,yaml]
----
node.name: data1
node.master: false
----

Diagnosing:

----
$ curl localhost:9200
$ curl localhost:9200/_cluster/health
----

Installing logstash on master node:

----
$ wget https://artifacts.elastic.co/downloads/logstash/logstash-6.2.3.rpm
$ sudo rpm --install logstash-6.2.3.rpm
$ sudo systemctl enable logstash
----

Configuration in `/etc/logstash/`:

* conf.d - directory for pipeline configuration files
* jvm.options
* log4j2.properties
* logstash.yml

Sample conf file:

----
input {
  beats {
    port => 5044
    host => "0.0.0.0"
  }
}
filter {
  if [fileset][module] == "apache2" {
    if [fileset][name] == "access" {
      grok {
        match => { "message" => ["%{IPORHOST:[apache2][access][remote_ip]} - %{DATA:[apache2][access][user_name]} \[%{HTTPDATE:[apache2][access][time]}\] \"%{WORD:[apache2][access][method]} %{DATA:[apache2][access][url]} HTTP/%{NUMBER:[apache2][access][http_version]}\" %{NUMBER:[apache2][access][response_code]} %{NUMBER:[apache2][access][body_sent][bytes]}( \"%{DATA:[apache2][access][referrer]}\")?( \"%{DATA:[apache2][access][agent]}\")?",
          "%{IPORHOST:[apache2][access][remote_ip]} - %{DATA:[apache2][access][user_name]} \\[%{HTTPDATE:[apache2][access][time]}\\] \"-\" %{NUMBER:[apache2][access][response_code]} -" ] }
        remove_field => "message"
      }
      mutate {
        add_field => { "read_timestamp" => "%{@timestamp}" }
      }
      date {
        match => [ "[apache2][access][time]", "dd/MMM/YYYY:H:m:s Z" ]
        remove_field => "[apache2][access][time]"
      }
      useragent {
        source => "[apache2][access][agent]"
        target => "[apache2][access][user_agent]"
        remove_field => "[apache2][access][agent]"
      }
      geoip {
        source => "[apache2][access][remote_ip]"
        target => "[apache2][access][geoip]"
      }
    }
    else if [fileset][name] == "error" {
      grok {
        match => { "message" => ["\[%{APACHE_TIME:[apache2][error][timestamp]}\] \[%{LOGLEVEL:[apache2][error][level]}\]( \[client %{IPORHOST:[apache2][error][client]}\])? %{GREEDYDATA:[apache2][error][message]}",
          "\[%{APACHE_TIME:[apache2][error][timestamp]}\] \[%{DATA:[apache2][error][module]}:%{LOGLEVEL:[apache2][error][level]}\] \[pid %{NUMBER:[apache2][error][pid]}(:tid %{NUMBER:[apache2][error][tid]})?\]( \[client %{IPORHOST:[apache2][error][client]}\])? %{GREEDYDATA:[apache2][error][message1]}" ] }
        pattern_definitions => {
          "APACHE_TIME" => "%{DAY} %{MONTH} %{MONTHDAY} %{TIME} %{YEAR}"
        }
        remove_field => "message"
      }
      mutate {
        rename => { "[apache2][error][message1]" => "[apache2][error][message]" }
      }
      date {
        match => [ "[apache2][error][timestamp]", "EEE MMM dd H:m:s YYYY", "EEE MMM dd H:m:s.SSSSSS YYYY" ]
        remove_field => "[apache2][error][timestamp]"
      }
    }
  }
}
output {
  elasticsearch {
    hosts => localhost
    manage_template => false
    index => "%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}"
  }
}
----

=== Filebeat

----
$ wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.2.3-x86_64.rpm
$ sudo rpm --install filebeat-6.2.3-x86_64.rpm
$ sudo systemctl enable filebeat
$ sudo filebeat setup
$ sudo vim /etc/filebeat/filebeat.yml
----

filebeat.yml:

----
ouput.logstash:
    hosts: ["localhost:5044"]
----

----
$ filebeat modules enable apache2
$ systemctl start filebeat
----
