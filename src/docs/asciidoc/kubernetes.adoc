= Kubernetes
:doc-root: https://notes.jdata.pl
:toc: left
:toclevels: 4
:tabsize: 4
:docinfo1:

docker-compose.yml

[source]
----
pg:
    image: postgres:9.4
frontend:
    image: nginx
    links:
        -pg
----

Another docker-compose.yml:

[source]
----
pg:
    image: postgres
api:
    restart: on-failure
    build: ./api
    volumes:
        - ./api:/app
    command: "--reload"
    links:
        - pg
    ports:
        - "8000:8000"
apiambassador:
    image: cpuguy83/docker-grand-ambassador
    volumes: /var/run/docker.sock:/var/run/docker.sock
    command: -name=chapter2_api_1
frontend:
    image: nginx
    volume:
        - ./frontend/nginx.conf:/etc/nginx/nginx.conf
    linx:
        - apiambassador:api
    ports:
        - "80:80"
----

An ambassador container is a container that stands in front of another one.

== What is Kubernetes

* runs docker containers
* labeling system for control
* spans many physical machines
* runs google container engine

.Pods:
* atomic unit of control
* set of running docker containers (often only one)
* containers "think" theyâ€™re running on the same machine
* always on the same host

.Replication controller (RC):
* manages pods
* tries to maintain a number of matching pods
* matches using labeling system

.Replication controller properties:
*count
*name
*version

.Services:
* DNS
* load balancer (set of pods comes from label matching)

=== Download kubernetes

Download latest stable release from https://github.com/kubernetes/kubernetes/releases

Open up cluster/aws/config-default.sh with editor

 $ KUBERNETES_PROVIDER=aws cluster/kube-up.sh

What that did:

* new VPC: 172.20.0.*
* 5 EC2 instances (master + 4 minions), with public IPs
* ASG for minions
* SSH keys for direct access
    ** ~/.ssh/kube_aws_rsa
* kubectl is configured:
    ** ~/.kube/config

=== Kubernetes service

* Service
* Endpoints

[source]
----
kind: Service
apiVersion: v1
metadata:
    name: database
spec:
    ports:
        - port: 5432
kind: Endpoints
apiVersion: v1
metadata:
    name: database
subnets:
    - addresses:
        - ip: 172.20.2.118
    ports:
        - port: 5432
----

 $ kubectl create -f database.yml

 $ kubectl describe svc database

== From start

=== Starting the cluster

 $ kubeadm config images pull

 $ sudo kubeadm init --pod-network-cidr=10.244.0.0/16

 $ mkdir ~/.kube
 $ sudo cp /etc/kubernetes/admin.conf ~/.kube/config
 $ sudo chown $(id -u):$(id -g) $HOME/.kube/config

 $ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml

 $ kubectl get pods --all-namespaces

 $ sudo kubeadm join 172.31.99.111:6443 --token 0wso1u.vxe0tt68vv0bshqj --discovery-token-ca-cert-hash sha256:c2a44af9e5182df3c35ab9c4e1811ad77e591ba5036991cd17c2ab9ce1370979

 $ kubectl get nodes

.Node types:
* master node
    ** kube-apiserver
        *** etcd
        *** kube-scheduler
        *** cloud-controller-manager
        *** kube-controller-mamager
* worker node
    ** kubelet
    ** kube proxy
    ** pod
        *** container

=== Debian Install From Scratch

.Ports (all are TCP ports):

[cols="^2,^2,10",options="header"]
|======
|node type  |ports       |usage
.6+|master     |6443        |Kubernetes API server
               |2379-2380   |etcd server client API
               |10250       |Kubelet API
               |10251       |kube-scheduler
               |10252       |kube-controller-manager
               |10255       |Read-Only Kubelet API
.3+|worker     |10250       |Kubelet API
               |10255       |Read-Only Kubelet API
               |30000-32767 |NodePort Services
|======

 $ sudo -i
 $ sudo apt install docker.io -y

./etc/docker/daemon.json
[source]
{
    "exec-opts": ["native.cgroupdriver=systemd"]
}

 $ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -

./etc/apt/sources.list.d/kubernetes.list
[source]
deb http://apt.kubernetes.io/ kubernetes-xenial main

 $ sudo apt update && sudo apt upgrade -y

 $ sudo apt install -y kubelet kubeadm kubectl

 $ sudo kubeadm init --pod-network-cidr=10.244.0.0/16

Exit the root shell

 $ mkdir $HOME/.kube
 $ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
 $ sudo chown $(id -u):$(id -g) $HOME/.kube/config

 $ kubectl get pods --all-namespaces

 $ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml

==== Creating worker nodes

 $ sudo apt install docker.io -y

./etc/docker/daemon.json
[source]
{
    "exec-opts": ["native.cgroupdriver=systemd"]
}

 $ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -

./etc/apt/sources.list.d/kubernetes.list
[source]
deb http://apt.kubernetes.io/ kubernetes-xenial main

 $ sudo apt update && sudo apt upgrade -y

 $ sudo apt install -y kubelet kubeadm kubectl

 $ kubeadm join ...

=== Installing on RedHat

.swap off:

 $ sudo swapoff -a

Remove swap from `/etc/fstab

 $ sudo yum update -y

 $ sudo yum -y install docker

 $ sudo systemctl enable docker

 $ sudo systemctl start docker

[source]
----
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF
----

 $ sudo setenforce 0

.edit /etc/selinux/config

 SELINUX=permissive

 $ sudo yum install -y kubelet kubeadm kubectl

 $ sudo systemctl enable kubelet

 $ sudo systemctl start kubelet

[source]
----
cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
----

 $ sudo sysctl --system

 $ sudo kubeadm init --pod-network-cidr=10.244.0.0/16

 $ mkdir -p $HOME/.kube
 $ sudo cp /etc/kubernetes/admin.conf $HOME/.kube/config
 $ sudo chmod $(id -u):$(id -g) config

 $ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml


=== Container networking

* CNM - Container Network Model
* CNI - Container Network Interface

==== Container Network Model

Specification proposed by Docker.

Adopted by libnetwork

.Native Dorker drivers:
* none
* bridge
* overlay

`Libnetwork` is the canonical implementation of the CNM specification.

